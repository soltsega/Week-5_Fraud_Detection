{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "643d6224",
   "metadata": {},
   "source": [
    "## Model Explainability with SHAP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5568f8fd",
   "metadata": {},
   "source": [
    "# Task 3: Model Explainability - Step-by-Step To-Do List\n",
    "\n",
    "## Phase 1: Setup & Data Preparation\n",
    "1. **Environment Setup**\n",
    "   - [ ] Install SHAP: `pip install shap`\n",
    "   - [ ] Import required libraries\n",
    "   - [ ] Create necessary directories\n",
    "\n",
    "2. **Load Model & Data**\n",
    "   - [ ] Load trained XGBoost model\n",
    "   - [ ] Load test dataset\n",
    "   - [ ] Verify data shapes and types\n",
    "\n",
    "## Phase 2: Feature Importance Analysis\n",
    "3. **Built-in Feature Importance**\n",
    "   - [ ] Extract feature importances from XGBoost\n",
    "   - [ ] Create bar plot of top 10 features\n",
    "   - [ ] Save visualization\n",
    "\n",
    "## Phase 3: SHAP Analysis\n",
    "4. **Global Explainability**\n",
    "   - [ ] Initialize SHAP explainer\n",
    "   - [ ] Calculate SHAP values (sample if needed for performance)\n",
    "   - [ ] Generate SHAP summary plot\n",
    "   - [ ] Save visualization\n",
    "\n",
    "5. **Local Explainability**\n",
    "   - [ ] Identify example predictions:\n",
    "     - [ ] 1 True Positive\n",
    "     - [ ] 1 False Positive\n",
    "     - [ ] 1 False Negative\n",
    "   - [ ] Create SHAP force plots for each\n",
    "   - [ ] Save visualizations\n",
    "\n",
    "## Phase 4: Interpretation\n",
    "6. **Compare Feature Importance Methods**\n",
    "   - [ ] Create comparison table\n",
    "   - [ ] Document top 5 fraud drivers\n",
    "\n",
    "7. **Business Recommendations**\n",
    "   - [ ] List 3+ actionable recommendations\n",
    "   - [ ] Connect to SHAP insights\n",
    "   - [ ] Add potential business impact\n",
    "\n",
    "## Phase 5: Documentation\n",
    "8. **Update Notebook**\n",
    "   - [ ] Add clear section headers\n",
    "   - [ ] Include markdown explanations\n",
    "   - [ ] Add figure captions\n",
    "\n",
    "9. **Repository Updates**\n",
    "   - [ ] Update README\n",
    "   - [ ] Ensure all paths are relative\n",
    "   - [ ] Add requirements.txt if missing\n",
    "\n",
    "## Phase 6: Final Checks\n",
    "10. **Verification**\n",
    "    - [ ] Verify all visualizations are clear\n",
    "    - [ ] Ensure code is well-commented\n",
    "    - [ ] Cross-validate findings\n",
    "    - [ ] Commit and push final changes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867d9bb5",
   "metadata": {},
   "source": [
    "## Environment setup\n",
    "\n",
    "Install required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dbf48704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Setting up environment for SHAP analysis...\n",
      "Requirement already satisfied: shap in c:\\users\\my device\\desktop\\week-5_kaim\\mvenv\\lib\\site-packages (0.50.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\my device\\desktop\\week-5_kaim\\mvenv\\lib\\site-packages (2.3.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\my device\\desktop\\week-5_kaim\\mvenv\\lib\\site-packages (2.3.5)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\my device\\desktop\\week-5_kaim\\mvenv\\lib\\site-packages (3.10.8)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\my device\\desktop\\week-5_kaim\\mvenv\\lib\\site-packages (1.8.0)\n",
      "Requirement already satisfied: scipy in c:\\users\\my device\\desktop\\week-5_kaim\\mvenv\\lib\\site-packages (from shap) (1.16.3)\n",
      "Requirement already satisfied: tqdm>=4.27.0 in c:\\users\\my device\\desktop\\week-5_kaim\\mvenv\\lib\\site-packages (from shap) (4.67.1)\n",
      "Requirement already satisfied: packaging>20.9 in c:\\users\\my device\\desktop\\week-5_kaim\\mvenv\\lib\\site-packages (from shap) (25.0)\n",
      "Requirement already satisfied: slicer==0.0.8 in c:\\users\\my device\\desktop\\week-5_kaim\\mvenv\\lib\\site-packages (from shap) (0.0.8)\n",
      "Requirement already satisfied: numba==0.63.0b1 in c:\\users\\my device\\desktop\\week-5_kaim\\mvenv\\lib\\site-packages (from shap) (0.63.0b1)\n",
      "Requirement already satisfied: llvmlite==0.46.0b1 in c:\\users\\my device\\desktop\\week-5_kaim\\mvenv\\lib\\site-packages (from shap) (0.46.0b1)\n",
      "Requirement already satisfied: cloudpickle in c:\\users\\my device\\desktop\\week-5_kaim\\mvenv\\lib\\site-packages (from shap) (3.1.2)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\my device\\desktop\\week-5_kaim\\mvenv\\lib\\site-packages (from shap) (4.15.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\my device\\desktop\\week-5_kaim\\mvenv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\my device\\desktop\\week-5_kaim\\mvenv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\my device\\desktop\\week-5_kaim\\mvenv\\lib\\site-packages (from pandas) (2025.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\my device\\desktop\\week-5_kaim\\mvenv\\lib\\site-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\my device\\desktop\\week-5_kaim\\mvenv\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\my device\\desktop\\week-5_kaim\\mvenv\\lib\\site-packages (from matplotlib) (4.61.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\my device\\desktop\\week-5_kaim\\mvenv\\lib\\site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\my device\\desktop\\week-5_kaim\\mvenv\\lib\\site-packages (from matplotlib) (12.0.0)\n",
      "Requirement already satisfied: pyparsing>=3 in c:\\users\\my device\\desktop\\week-5_kaim\\mvenv\\lib\\site-packages (from matplotlib) (3.2.5)\n",
      "Requirement already satisfied: joblib>=1.3.0 in c:\\users\\my device\\desktop\\week-5_kaim\\mvenv\\lib\\site-packages (from scikit-learn) (1.5.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.2.0 in c:\\users\\my device\\desktop\\week-5_kaim\\mvenv\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\my device\\desktop\\week-5_kaim\\mvenv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\my device\\desktop\\week-5_kaim\\mvenv\\lib\\site-packages (from tqdm>=4.27.0->shap) (0.4.6)\n",
      "‚úÖ Environment setup complete!\n",
      "Notebook directory: c:\\Users\\My Device\\Desktop\\Week-5_KAIM\\fraud-detection\\notebooks\n",
      "Data directory: c:\\Users\\My Device\\Desktop\\Week-5_KAIM\\fraud-detection\\data\\processed\n",
      "Model directory: c:\\Users\\My Device\\Desktop\\Week-5_KAIM\\fraud-detection\\models\n",
      "Reports directory: c:\\Users\\My Device\\Desktop\\Week-5_KAIM\\fraud-detection\\reports\\figures\n",
      "\n",
      "‚úÖ Environment is ready for SHAP analysis!\n",
      "Next step: Loading the model and data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# ===========================================\n",
    "# SHAP Model Explainability - Environment Setup\n",
    "# ===========================================\n",
    "\n",
    "print(\"üîß Setting up environment for SHAP analysis...\")\n",
    "\n",
    "# 1. Install required packages\n",
    "!pip install shap pandas numpy matplotlib scikit-learn\n",
    "\n",
    "# 2. Import necessary libraries\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import os\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# 3. Set up paths\n",
    "NOTEBOOK_DIR = Path.cwd()\n",
    "ROOT_DIR = NOTEBOOK_DIR.parent\n",
    "DATA_DIR = ROOT_DIR / \"data\" / \"processed\"\n",
    "MODEL_DIR = ROOT_DIR / \"models\"\n",
    "REPORTS_DIR = ROOT_DIR / \"reports\" / \"figures\"\n",
    "\n",
    "# 4. Create directories if they don't exist\n",
    "REPORTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"‚úÖ Environment setup complete!\")\n",
    "print(f\"Notebook directory: {NOTEBOOK_DIR}\")\n",
    "print(f\"Data directory: {DATA_DIR}\")\n",
    "print(f\"Model directory: {MODEL_DIR}\")\n",
    "print(f\"Reports directory: {REPORTS_DIR}\")\n",
    "\n",
    "# 5. Set display options for better readability\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.width', 1000)\n",
    "plt.style.use('seaborn-v0_8')  # Updated style for newer matplotlib versions\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"\\n‚úÖ Environment is ready for SHAP analysis!\")\n",
    "print(\"Next step: Loading the model and data...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b090a1f0",
   "metadata": {},
   "source": [
    "## Loading the model and the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f369cf96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Checking available model files...\n",
      "üìã Available model files:\n",
      "   1. fraud_detection_xgboost_v1_20251227.pkl (Size: 0.00 MB)\n",
      "   2. MODEL_CARD.md (Size: 0.00 MB)\n",
      "   3. model_metadata_v1.json (Size: 0.00 MB)\n",
      "\n",
      "üìã Files in parent directory:\n",
      "   1. .gitignore (Size: 0.00 MB)\n",
      "   2. data (Size: 0.00 MB)\n",
      "   3. models (Size: 0.00 MB)\n",
      "   4. notebooks (Size: 0.00 MB)\n",
      "   5. README.md (Size: 0.00 MB)\n",
      "   6. reports (Size: 0.00 MB)\n",
      "   7. results (Size: 0.00 MB)\n",
      "   8. scripts (Size: 0.00 MB)\n",
      "   9. src (Size: 0.00 MB)\n",
      "   10. tests (Size: 0.00 MB)\n",
      "\n",
      "Please provide the correct model filename from the list above.\n"
     ]
    }
   ],
   "source": [
    "# ===========================================\n",
    "# Check Available Model Files\n",
    "# ===========================================\n",
    "\n",
    "print(\"üîç Checking available model files...\")\n",
    "\n",
    "# List all files in the models directory\n",
    "model_files = list(MODEL_DIR.glob(\"*\"))\n",
    "if model_files:\n",
    "    print(\"üìã Available model files:\")\n",
    "    for i, file in enumerate(model_files, 1):\n",
    "        print(f\"   {i}. {file.name} (Size: {file.stat().st_size / (1024*1024):.2f} MB)\")\n",
    "else:\n",
    "    print(\"‚ùå No files found in the models directory.\")\n",
    "    print(f\"   Directory path: {MODEL_DIR}\")\n",
    "\n",
    "# Also check the parent directory in case the model is there\n",
    "parent_dir_files = list((MODEL_DIR.parent).glob(\"*\"))\n",
    "print(\"\\nüìã Files in parent directory:\")\n",
    "for i, file in enumerate(parent_dir_files, 1):\n",
    "    print(f\"   {i}. {file.name} (Size: {file.stat().st_size / (1024*1024):.2f} MB)\")\n",
    "\n",
    "print(\"\\nPlease provide the correct model filename from the list above.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4fd5f42e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Loading model and data...\n",
      "‚úÖ Model loaded successfully from: c:\\Users\\My Device\\Desktop\\Week-5_KAIM\\fraud-detection\\models\\fraud_detection_xgboost_v1_20251227.pkl\n",
      "   Model type: XGBClassifier\n",
      "\n",
      "Model parameters:\n",
      "----------------------------------------\n",
      "objective: binary:logistic\n",
      "base_score: None\n",
      "booster: None\n",
      "callbacks: None\n",
      "colsample_bylevel: None\n",
      "colsample_bynode: None\n",
      "colsample_bytree: 0.8\n",
      "device: None\n",
      "early_stopping_rounds: None\n",
      "enable_categorical: False\n",
      "eval_metric: aucpr\n",
      "feature_types: None\n",
      "feature_weights: None\n",
      "gamma: None\n",
      "grow_policy: None\n",
      "importance_type: None\n",
      "interaction_constraints: None\n",
      "learning_rate: 0.05\n",
      "max_bin: None\n",
      "max_cat_threshold: None\n",
      "max_cat_to_onehot: None\n",
      "max_delta_step: None\n",
      "max_depth: 6\n",
      "max_leaves: None\n",
      "min_child_weight: None\n",
      "missing: nan\n",
      "monotone_constraints: None\n",
      "multi_strategy: None\n",
      "n_estimators: 1000\n",
      "n_jobs: -1\n",
      "num_parallel_tree: None\n",
      "random_state: 42\n",
      "reg_alpha: None\n",
      "reg_lambda: None\n",
      "sampling_method: None\n",
      "scale_pos_weight: 577.2868020304569\n",
      "subsample: 0.8\n",
      "tree_method: None\n",
      "validate_parameters: None\n",
      "verbosity: None\n",
      "----------------------------------------\n",
      "\n",
      "‚ùå Error loading test data: [Errno 2] No such file or directory: 'c:\\\\Users\\\\My Device\\\\Desktop\\\\Week-5_KAIM\\\\fraud-detection\\\\data\\\\processed\\\\test_data.csv'\n",
      "\n",
      "üìã Available files in data directory:\n",
      "   - cc_X_test.csv\n",
      "   - cc_X_train.csv\n",
      "   - cc_y_test.csv\n",
      "   - cc_y_train.csv\n",
      "   - creditcard_enhanced.csv\n",
      "   - fd_X_test.csv\n",
      "   - fd_X_train.csv\n",
      "   - fd_y_test.csv\n",
      "   - fd_y_train.csv\n",
      "   - fraud_data_with_behavior_features.csv\n",
      "   - fraud_data_with_country_final.csv\n",
      "   - fraud_data_with_device_features.csv\n",
      "   - fraud_data_with_geo_features.csv\n",
      "   - fraud_data_with_time_features.csv\n",
      "   - processed_data_undersampled.joblib\n",
      "\n",
      "Please provide the correct test data filename from the list above.\n"
     ]
    }
   ],
   "source": [
    "# ===========================================\n",
    "# Loading Model and Data\n",
    "# ===========================================\n",
    "\n",
    "print(\"üìÇ Loading model and data...\")\n",
    "\n",
    "# 1. Load the trained model\n",
    "try:\n",
    "    model_path = MODEL_DIR / \"fraud_detection_xgboost_v1_20251227.pkl\"  # Updated filename\n",
    "    model = joblib.load(model_path)\n",
    "    print(f\"‚úÖ Model loaded successfully from: {model_path}\")\n",
    "    print(f\"   Model type: {type(model).__name__}\")\n",
    "    \n",
    "    # Print model parameters for verification\n",
    "    print(\"\\nModel parameters:\")\n",
    "    print(\"-\" * 40)\n",
    "    for param, value in model.get_params().items():\n",
    "        print(f\"{param}: {value}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading model: {e}\")\n",
    "    print(\"\\nüìã Available files in models directory:\")\n",
    "    print(\"\\n\".join([f\"   - {f.name}\" for f in MODEL_DIR.glob(\"*\")]))\n",
    "    raise\n",
    "\n",
    "# 2. Load the test data\n",
    "try:\n",
    "    test_data_path = DATA_DIR / \"test_data.csv\"  # We'll verify this next\n",
    "    test_data = pd.read_csv(test_data_path)\n",
    "    \n",
    "    # Separate features and target\n",
    "    X_test = test_data.drop('Class', axis=1)\n",
    "    y_test = test_data['Class']\n",
    "    \n",
    "    print(f\"\\n‚úÖ Test data loaded: {len(X_test)} samples\")\n",
    "    print(f\"   Features: {X_test.shape[1]}\")\n",
    "    print(\"\\nClass distribution:\")\n",
    "    print(y_test.value_counts().to_string())\n",
    "    \n",
    "    # Display first few rows of features\n",
    "    print(\"\\nFirst few rows of features:\")\n",
    "    display(X_test.head())\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Error loading test data: {e}\")\n",
    "    print(\"\\nüìã Available files in data directory:\")\n",
    "    print(\"\\n\".join([f\"   - {f.name}\" for f in DATA_DIR.glob(\"*\")]))\n",
    "    print(\"\\nPlease provide the correct test data filename from the list above.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f409c30e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Loading model and data...\n",
      "‚úÖ Model already loaded successfully\n",
      "\n",
      "‚úÖ Test data loaded successfully\n",
      "   Features shape: (56962, 30)\n",
      "   Target shape: (56962,)\n",
      "\n",
      "Class distribution in test set:\n",
      "Class\n",
      "0    56864\n",
      "1       98\n",
      "\n",
      "First few rows of features:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>V11</th>\n",
       "      <th>V12</th>\n",
       "      <th>V13</th>\n",
       "      <th>V14</th>\n",
       "      <th>V15</th>\n",
       "      <th>V16</th>\n",
       "      <th>V17</th>\n",
       "      <th>V18</th>\n",
       "      <th>V19</th>\n",
       "      <th>V20</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>160760.0</td>\n",
       "      <td>-0.674466</td>\n",
       "      <td>1.408105</td>\n",
       "      <td>-1.110622</td>\n",
       "      <td>-1.328366</td>\n",
       "      <td>1.388996</td>\n",
       "      <td>-1.308439</td>\n",
       "      <td>1.885879</td>\n",
       "      <td>-0.614233</td>\n",
       "      <td>0.311652</td>\n",
       "      <td>0.650757</td>\n",
       "      <td>-0.857785</td>\n",
       "      <td>-0.229961</td>\n",
       "      <td>-0.199817</td>\n",
       "      <td>0.266371</td>\n",
       "      <td>-0.046544</td>\n",
       "      <td>-0.741398</td>\n",
       "      <td>-0.605617</td>\n",
       "      <td>-0.392568</td>\n",
       "      <td>-0.162648</td>\n",
       "      <td>0.394322</td>\n",
       "      <td>0.080084</td>\n",
       "      <td>0.810034</td>\n",
       "      <td>-0.224327</td>\n",
       "      <td>0.707899</td>\n",
       "      <td>-0.135837</td>\n",
       "      <td>0.045102</td>\n",
       "      <td>0.533837</td>\n",
       "      <td>0.291319</td>\n",
       "      <td>23.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19847.0</td>\n",
       "      <td>-2.829816</td>\n",
       "      <td>-2.765149</td>\n",
       "      <td>2.537793</td>\n",
       "      <td>-1.074580</td>\n",
       "      <td>2.842559</td>\n",
       "      <td>-2.153536</td>\n",
       "      <td>-1.795519</td>\n",
       "      <td>-0.250020</td>\n",
       "      <td>3.073504</td>\n",
       "      <td>-1.000418</td>\n",
       "      <td>1.850842</td>\n",
       "      <td>-1.549779</td>\n",
       "      <td>1.252337</td>\n",
       "      <td>0.963974</td>\n",
       "      <td>-0.481027</td>\n",
       "      <td>-0.147319</td>\n",
       "      <td>-0.209328</td>\n",
       "      <td>1.058898</td>\n",
       "      <td>0.397057</td>\n",
       "      <td>-0.515765</td>\n",
       "      <td>-0.295555</td>\n",
       "      <td>0.109305</td>\n",
       "      <td>-0.813272</td>\n",
       "      <td>0.042996</td>\n",
       "      <td>-0.027660</td>\n",
       "      <td>-0.910247</td>\n",
       "      <td>0.110802</td>\n",
       "      <td>-0.511938</td>\n",
       "      <td>11.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>88326.0</td>\n",
       "      <td>-3.576495</td>\n",
       "      <td>2.318422</td>\n",
       "      <td>1.306985</td>\n",
       "      <td>3.263665</td>\n",
       "      <td>1.127818</td>\n",
       "      <td>2.865246</td>\n",
       "      <td>1.444125</td>\n",
       "      <td>-0.718922</td>\n",
       "      <td>1.874046</td>\n",
       "      <td>7.398491</td>\n",
       "      <td>2.081146</td>\n",
       "      <td>-0.064145</td>\n",
       "      <td>0.577556</td>\n",
       "      <td>-2.430201</td>\n",
       "      <td>1.505993</td>\n",
       "      <td>-1.237941</td>\n",
       "      <td>-0.390405</td>\n",
       "      <td>-1.231804</td>\n",
       "      <td>0.098738</td>\n",
       "      <td>2.034786</td>\n",
       "      <td>-1.060151</td>\n",
       "      <td>0.016867</td>\n",
       "      <td>-0.132058</td>\n",
       "      <td>-1.483996</td>\n",
       "      <td>-0.296011</td>\n",
       "      <td>0.062823</td>\n",
       "      <td>0.552411</td>\n",
       "      <td>0.509764</td>\n",
       "      <td>76.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>141734.0</td>\n",
       "      <td>2.060386</td>\n",
       "      <td>-0.015382</td>\n",
       "      <td>-1.082544</td>\n",
       "      <td>0.386019</td>\n",
       "      <td>-0.024331</td>\n",
       "      <td>-1.074935</td>\n",
       "      <td>0.207792</td>\n",
       "      <td>-0.338140</td>\n",
       "      <td>0.455091</td>\n",
       "      <td>0.047859</td>\n",
       "      <td>-0.652497</td>\n",
       "      <td>0.750829</td>\n",
       "      <td>0.665603</td>\n",
       "      <td>0.158608</td>\n",
       "      <td>0.027348</td>\n",
       "      <td>-0.171173</td>\n",
       "      <td>-0.291228</td>\n",
       "      <td>-1.008531</td>\n",
       "      <td>0.097040</td>\n",
       "      <td>-0.192024</td>\n",
       "      <td>-0.281684</td>\n",
       "      <td>-0.639426</td>\n",
       "      <td>0.331818</td>\n",
       "      <td>-0.067584</td>\n",
       "      <td>-0.283675</td>\n",
       "      <td>0.203529</td>\n",
       "      <td>-0.063621</td>\n",
       "      <td>-0.060077</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>38741.0</td>\n",
       "      <td>1.209965</td>\n",
       "      <td>1.384303</td>\n",
       "      <td>-1.343531</td>\n",
       "      <td>1.763636</td>\n",
       "      <td>0.662351</td>\n",
       "      <td>-2.113384</td>\n",
       "      <td>0.854039</td>\n",
       "      <td>-0.475963</td>\n",
       "      <td>-0.629658</td>\n",
       "      <td>-1.579654</td>\n",
       "      <td>1.462573</td>\n",
       "      <td>0.208823</td>\n",
       "      <td>0.734537</td>\n",
       "      <td>-3.538625</td>\n",
       "      <td>0.926076</td>\n",
       "      <td>0.835029</td>\n",
       "      <td>2.845937</td>\n",
       "      <td>1.040947</td>\n",
       "      <td>-1.045263</td>\n",
       "      <td>0.009083</td>\n",
       "      <td>-0.164015</td>\n",
       "      <td>-0.328294</td>\n",
       "      <td>-0.154631</td>\n",
       "      <td>0.619449</td>\n",
       "      <td>0.818998</td>\n",
       "      <td>-0.330525</td>\n",
       "      <td>0.046884</td>\n",
       "      <td>0.104527</td>\n",
       "      <td>1.50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Time        V1        V2        V3        V4        V5        V6        V7        V8        V9       V10       V11       V12       V13       V14       V15       V16       V17       V18       V19       V20       V21       V22       V23       V24       V25       V26       V27       V28  Amount\n",
       "0  160760.0 -0.674466  1.408105 -1.110622 -1.328366  1.388996 -1.308439  1.885879 -0.614233  0.311652  0.650757 -0.857785 -0.229961 -0.199817  0.266371 -0.046544 -0.741398 -0.605617 -0.392568 -0.162648  0.394322  0.080084  0.810034 -0.224327  0.707899 -0.135837  0.045102  0.533837  0.291319   23.00\n",
       "1   19847.0 -2.829816 -2.765149  2.537793 -1.074580  2.842559 -2.153536 -1.795519 -0.250020  3.073504 -1.000418  1.850842 -1.549779  1.252337  0.963974 -0.481027 -0.147319 -0.209328  1.058898  0.397057 -0.515765 -0.295555  0.109305 -0.813272  0.042996 -0.027660 -0.910247  0.110802 -0.511938   11.85\n",
       "2   88326.0 -3.576495  2.318422  1.306985  3.263665  1.127818  2.865246  1.444125 -0.718922  1.874046  7.398491  2.081146 -0.064145  0.577556 -2.430201  1.505993 -1.237941 -0.390405 -1.231804  0.098738  2.034786 -1.060151  0.016867 -0.132058 -1.483996 -0.296011  0.062823  0.552411  0.509764   76.07\n",
       "3  141734.0  2.060386 -0.015382 -1.082544  0.386019 -0.024331 -1.074935  0.207792 -0.338140  0.455091  0.047859 -0.652497  0.750829  0.665603  0.158608  0.027348 -0.171173 -0.291228 -1.008531  0.097040 -0.192024 -0.281684 -0.639426  0.331818 -0.067584 -0.283675  0.203529 -0.063621 -0.060077    0.99\n",
       "4   38741.0  1.209965  1.384303 -1.343531  1.763636  0.662351 -2.113384  0.854039 -0.475963 -0.629658 -1.579654  1.462573  0.208823  0.734537 -3.538625  0.926076  0.835029  2.845937  1.040947 -1.045263  0.009083 -0.164015 -0.328294 -0.154631  0.619449  0.818998 -0.330525  0.046884  0.104527    1.50"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature verification:\n",
      "Number of features: 30\n",
      "First 5 feature names: ['Time', 'V1', 'V2', 'V3', 'V4']\n",
      "\n",
      "‚úÖ Data loading completed successfully!\n",
      "\n",
      "Next step: Initializing SHAP explainer...\n"
     ]
    }
   ],
   "source": [
    "# ===========================================\n",
    "# Loading Model and Data\n",
    "# ===========================================\n",
    "\n",
    "print(\"üìÇ Loading model and data...\")\n",
    "\n",
    "# 1. Load the trained model (already loaded successfully)\n",
    "print(\"‚úÖ Model already loaded successfully\")\n",
    "\n",
    "# 2. Load the test data\n",
    "try:\n",
    "    # Load features and target separately\n",
    "    X_test = pd.read_csv(DATA_DIR / \"cc_X_test.csv\")\n",
    "    y_test = pd.read_csv(DATA_DIR / \"cc_y_test.csv\")\n",
    "    \n",
    "    # If y_test is a DataFrame with a single column, convert to Series\n",
    "    if isinstance(y_test, pd.DataFrame) and len(y_test.columns) == 1:\n",
    "        y_test = y_test.iloc[:, 0]\n",
    "    \n",
    "    print(f\"\\n‚úÖ Test data loaded successfully\")\n",
    "    print(f\"   Features shape: {X_test.shape}\")\n",
    "    print(f\"   Target shape: {y_test.shape if hasattr(y_test, 'shape') else len(y_test)}\")\n",
    "    \n",
    "    # Display class distribution\n",
    "    print(\"\\nClass distribution in test set:\")\n",
    "    print(y_test.value_counts().to_string())\n",
    "    \n",
    "    # Display first few rows of features\n",
    "    print(\"\\nFirst few rows of features:\")\n",
    "    display(X_test.head())\n",
    "    \n",
    "    # Verify feature names match training\n",
    "    print(\"\\nFeature verification:\")\n",
    "    print(f\"Number of features: {X_test.shape[1]}\")\n",
    "    print(\"First 5 feature names:\", list(X_test.columns[:5]))\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Error loading test data: {e}\")\n",
    "    print(\"\\nüìã Available files in data directory:\")\n",
    "    print(\"\\n\".join([f\"   - {f.name}\" for f in DATA_DIR.glob(\"*\")]))\n",
    "    raise\n",
    "\n",
    "print(\"\\n‚úÖ Data loading completed successfully!\")\n",
    "print(\"\\nNext step: Initializing SHAP explainer...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbb1c17",
   "metadata": {},
   "source": [
    "## Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8adeb166",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Loading XGBoost model with Booster...\n",
      "‚úÖ Model loaded with joblib\n",
      "‚ö†Ô∏è Prediction test failed: need to call fit or load_model beforehand\n",
      "‚ùå Error loading model: need to call fit or load_model beforehand\n",
      "\n",
      "Troubleshooting steps:\n",
      "1. Let's verify the model file exists and is not corrupted\n",
      "2. Checking file size...\n",
      "   - File exists. Size: 0.00 MB\n"
     ]
    },
    {
     "ename": "NotFittedError",
     "evalue": "need to call fit or load_model beforehand",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNotFittedError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 42\u001b[39m\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     40\u001b[39m     \u001b[38;5;66;03m# Convert X_test to numpy array if it's a DataFrame\u001b[39;00m\n\u001b[32m     41\u001b[39m     X_test_array = X_test.values \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(X_test, \u001b[33m'\u001b[39m\u001b[33mvalues\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m X_test\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m     _ = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test_array\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Test with one sample\u001b[39;00m\n\u001b[32m     43\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m‚úÖ Model prediction test successful\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 32\u001b[39m, in \u001b[36mXGBoostWrapper.predict\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m     proba = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpredict_proba\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     33\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (proba > \u001b[32m0.5\u001b[39m).astype(\u001b[38;5;28mint\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 29\u001b[39m, in \u001b[36mXGBoostWrapper.predict_proba\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpredict_proba\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[32m     28\u001b[39m     dmatrix = xgb.DMatrix(X)\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdmatrix\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\My Device\\Desktop\\Week-5_KAIM\\mvenv\\Lib\\site-packages\\xgboost\\core.py:774\u001b[39m, in \u001b[36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    772\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig.parameters, args):\n\u001b[32m    773\u001b[39m     kwargs[k] = arg\n\u001b[32m--> \u001b[39m\u001b[32m774\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\My Device\\Desktop\\Week-5_KAIM\\mvenv\\Lib\\site-packages\\xgboost\\sklearn.py:1842\u001b[39m, in \u001b[36mXGBClassifier.predict\u001b[39m\u001b[34m(self, X, output_margin, validate_features, base_margin, iteration_range)\u001b[39m\n\u001b[32m   1831\u001b[39m \u001b[38;5;129m@_deprecate_positional_args\u001b[39m\n\u001b[32m   1832\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpredict\u001b[39m(\n\u001b[32m   1833\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1839\u001b[39m     iteration_range: Optional[IterationRange] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1840\u001b[39m ) -> ArrayLike:\n\u001b[32m   1841\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(verbosity=\u001b[38;5;28mself\u001b[39m.verbosity):\n\u001b[32m-> \u001b[39m\u001b[32m1842\u001b[39m         class_probs = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1843\u001b[39m \u001b[43m            \u001b[49m\u001b[43mX\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1844\u001b[39m \u001b[43m            \u001b[49m\u001b[43moutput_margin\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_margin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1845\u001b[39m \u001b[43m            \u001b[49m\u001b[43mvalidate_features\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalidate_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1846\u001b[39m \u001b[43m            \u001b[49m\u001b[43mbase_margin\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbase_margin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1847\u001b[39m \u001b[43m            \u001b[49m\u001b[43miteration_range\u001b[49m\u001b[43m=\u001b[49m\u001b[43miteration_range\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1848\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1849\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m output_margin:\n\u001b[32m   1850\u001b[39m             \u001b[38;5;66;03m# If output_margin is active, simply return the scores\u001b[39;00m\n\u001b[32m   1851\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m class_probs\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\My Device\\Desktop\\Week-5_KAIM\\mvenv\\Lib\\site-packages\\xgboost\\core.py:774\u001b[39m, in \u001b[36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    772\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig.parameters, args):\n\u001b[32m    773\u001b[39m     kwargs[k] = arg\n\u001b[32m--> \u001b[39m\u001b[32m774\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\My Device\\Desktop\\Week-5_KAIM\\mvenv\\Lib\\site-packages\\xgboost\\sklearn.py:1446\u001b[39m, in \u001b[36mXGBModel.predict\u001b[39m\u001b[34m(self, X, output_margin, validate_features, base_margin, iteration_range)\u001b[39m\n\u001b[32m   1444\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._can_use_inplace_predict():\n\u001b[32m   1445\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1446\u001b[39m         predts = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_booster\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m.inplace_predict(\n\u001b[32m   1447\u001b[39m             data=X,\n\u001b[32m   1448\u001b[39m             iteration_range=iteration_range,\n\u001b[32m   1449\u001b[39m             predict_type=\u001b[33m\"\u001b[39m\u001b[33mmargin\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_margin \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mvalue\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1450\u001b[39m             missing=\u001b[38;5;28mself\u001b[39m.missing,\n\u001b[32m   1451\u001b[39m             base_margin=base_margin,\n\u001b[32m   1452\u001b[39m             validate_features=validate_features,\n\u001b[32m   1453\u001b[39m         )\n\u001b[32m   1454\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m _is_cupy_alike(predts):\n\u001b[32m   1455\u001b[39m             cp = import_cupy()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\My Device\\Desktop\\Week-5_KAIM\\mvenv\\Lib\\site-packages\\xgboost\\sklearn.py:1010\u001b[39m, in \u001b[36mXGBModel.get_booster\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1007\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.__sklearn_is_fitted__():\n\u001b[32m   1008\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexceptions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m NotFittedError\n\u001b[32m-> \u001b[39m\u001b[32m1010\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m NotFittedError(\u001b[33m\"\u001b[39m\u001b[33mneed to call fit or load_model beforehand\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1011\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._Booster\n",
      "\u001b[31mNotFittedError\u001b[39m: need to call fit or load_model beforehand"
     ]
    }
   ],
   "source": [
    "# ===========================================\n",
    "# Load XGBoost Model with Booster\n",
    "# ===========================================\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "\n",
    "print(\"üîÑ Loading XGBoost model with Booster...\")\n",
    "try:\n",
    "    model_path = MODEL_DIR / \"fraud_detection_xgboost_v1_20251227.pkl\"\n",
    "    \n",
    "    # Try loading with joblib first\n",
    "    try:\n",
    "        model = joblib.load(model_path)\n",
    "        print(\"‚úÖ Model loaded with joblib\")\n",
    "    except:\n",
    "        # If joblib fails, try loading with XGBoost's Booster\n",
    "        model = xgb.Booster()\n",
    "        model.load_model(str(model_path))\n",
    "        print(\"‚úÖ Model loaded with XGBoost Booster\")\n",
    "    \n",
    "    # Create a wrapper class for prediction\n",
    "    class XGBoostWrapper:\n",
    "        def __init__(self, model):\n",
    "            self.model = model\n",
    "            self._fitted = True  # To bypass scikit-learn's check\n",
    "        \n",
    "        def predict_proba(self, X):\n",
    "            dmatrix = xgb.DMatrix(X)\n",
    "            return self.model.predict(dmatrix)\n",
    "            \n",
    "        def predict(self, X):\n",
    "            proba = self.predict_proba(X)\n",
    "            return (proba > 0.5).astype(int)\n",
    "    \n",
    "    # Wrap the model\n",
    "    model = XGBoostWrapper(model)\n",
    "    \n",
    "    # Test prediction\n",
    "    try:\n",
    "        # Convert X_test to numpy array if it's a DataFrame\n",
    "        X_test_array = X_test.values if hasattr(X_test, 'values') else X_test\n",
    "        _ = model.predict(X_test_array[:1])  # Test with one sample\n",
    "        print(\"‚úÖ Model prediction test successful\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Prediction test failed: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading model: {str(e)}\")\n",
    "    print(\"\\nTroubleshooting steps:\")\n",
    "    print(\"1. Let's verify the model file exists and is not corrupted\")\n",
    "    print(\"2. Checking file size...\")\n",
    "    if model_path.exists():\n",
    "        print(f\"   - File exists. Size: {model_path.stat().st_size / (1024*1024):.2f} MB\")\n",
    "    else:\n",
    "        print(\"   - File does not exist at the specified path\")\n",
    "    raise\n",
    "\n",
    "print(\"\\n‚úÖ Model loading completed. Ready for analysis!\")\n",
    "\n",
    "# Save the model wrapper for later use\n",
    "model_wrapper = model\n",
    "print(\"\\nModel wrapper created. You can use 'model_wrapper' for predictions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "19f00278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Analyzing Feature Importance using Booster...\n",
      "‚ö†Ô∏è Could not generate feature importance using booster: need to call fit or load_model beforehand\n",
      "Proceeding with SHAP values for feature importance...\n",
      "\n",
      "‚úÖ Feature importance analysis completed!\n",
      "\n",
      "Next step: Proceeding with SHAP analysis...\n"
     ]
    }
   ],
   "source": [
    "# ===========================================\n",
    "# Feature Importance Analysis - Alternative Approach\n",
    "# ===========================================\n",
    "print(\"üìä Analyzing Feature Importance using Booster...\")\n",
    "try:\n",
    "    # Try to get the booster object\n",
    "    if hasattr(model, 'get_booster'):\n",
    "        booster = model.get_booster()\n",
    "        # Get feature importance as a dictionary\n",
    "        importance_dict = booster.get_score(importance_type='gain')\n",
    "        \n",
    "        # Create a DataFrame for visualization\n",
    "        importance_df = pd.DataFrame({\n",
    "            'Feature': list(importance_dict.keys()),\n",
    "            'Importance': list(importance_dict.values())\n",
    "        }).sort_values('Importance', ascending=False)\n",
    "        \n",
    "        # Display top 20 features\n",
    "        print(\"\\nTop 20 Most Important Features (Gain):\")\n",
    "        display(importance_df.head(20))\n",
    "        \n",
    "        # Plot feature importance\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        sns.barplot(\n",
    "            x='Importance', \n",
    "            y='Feature', \n",
    "            data=importance_df.head(20),\n",
    "            palette='viridis'\n",
    "        )\n",
    "        plt.title('Top 20 Most Important Features (Gain)', fontsize=14)\n",
    "        plt.xlabel('Importance (Gain)', fontsize=12)\n",
    "        plt.ylabel('Feature', fontsize=12)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save the plot\n",
    "        importance_plot_path = REPORTS_DIR / \"feature_importance_gain.png\"\n",
    "        plt.savefig(importance_plot_path, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        print(f\"‚úÖ Feature importance (Gain) plot saved to: {importance_plot_path}\")\n",
    "        \n",
    "        # Save feature importances to CSV\n",
    "        importance_csv_path = REPORTS_DIR / \"feature_importances_gain.csv\"\n",
    "        importance_df.to_csv(importance_csv_path, index=False)\n",
    "        print(f\"‚úÖ Feature importances (Gain) saved to: {importance_csv_path}\")\n",
    "        \n",
    "    else:\n",
    "        print(\"‚ùå Could not access booster object. Trying alternative method...\")\n",
    "        raise AttributeError(\"Booster object not accessible\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Could not generate feature importance using booster: {str(e)}\")\n",
    "    print(\"Proceeding with SHAP values for feature importance...\")\n",
    "    use_shap_for_importance = True\n",
    "\n",
    "print(\"\\n‚úÖ Feature importance analysis completed!\")\n",
    "print(\"\\nNext step: Proceeding with SHAP analysis...\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
